{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERAL\n",
    "\n",
    "A Decision Tree is a supervised learning model used for both classification and regression tasks. It works by splitting the data into branches based on feature values, creating a tree-like structure where each internal node represents an outcome (class label or predictid value)\n",
    "\n",
    "In a Decision Tree algorithm, there is a tree like structure in which each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label. The paths from the root node to leaf node represent classification rules.\n",
    "\n",
    "* ***root node***: the starting point, containing all the data.\n",
    "* ***splitting***: the dataset is split based on the feature that provides the best separation (e.g., using *gini impurity* or *entropy* for classification).\n",
    "* ***decision nodes***: intermediate nodes that split the data further based on feature values.\n",
    "* ***leaf/terminal nodes***: the final nodes that provide the prediction (class label for classification or numerical value for regression).\n",
    "* ***pruning***: the removal of sub-nodes. It is the opposite process of splitting.\n",
    "* ***branch/sub-tree***: a sub-section of an entire tree is called a branch or sub-tree.\n",
    "* ***parent and child node***: a node, which is divided into sub-nodes is called the parent node of sub-nodes where sub-nodes are the children of a parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADVANTAGES\n",
    "* ***easy to understand and interpret***: decision trees provide clear visualizations that make them highly interpretable. The structure of the tree allows for easy understanding of how decisions are made at each node, which can be particularly useful for non-experts.\n",
    "\n",
    "* ***non-linear relationships:***: capable of modeling non-linear relationships between features and the target variable, decision trees offer greater flexibility compared to linear models that assume a linear connection between inputs and outputs.\n",
    "\n",
    "* ***handling of numerical and categorical data***: both numerical and categorical data can be processed without the need for additional preprocessing or encoding, distinguishing decision trees from other machine learning models that require feature scaling or transformation.\n",
    "\n",
    "* ***robustness to outliers:***: the model is relatively insensitive to outliers. Since splits are made based on feature values that separate the majority of data, extreme values do not heavily influence the modelâ€™s decision-making process.\n",
    "\n",
    "* ***automatic feature selection***: decision trees inherently perform feature selection by choosing the most relevant features for creating splits in the data, which can lead to reduced dimensionality and enhanced model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISADVANTAGES\n",
    "\n",
    "* ***prone to overfitting***: decision trees have a tendency to overfit, especially when the tree depth increases. A deep tree may capture noise and minor patterns, leading to poor generalization on new, unseen data.\n",
    "\n",
    "* ***instability***: decision trees can exhibit instability since slight changes in the data may result in a completely different structure. This is due to the greedy nature of the algorithm that selects the best split at each step, which can cause drastic changes in the final model.\n",
    "\n",
    "* ***bias towards dominant features***: the model can exhibit bias towards features with more categories or numerical splits. It may favor features that provide the most significant reduction in impurity, even if they are not the most informative in predicting the target variable.\n",
    "\n",
    "* ***difficulty modeling complex relationships***: while decision trees can handle non-linear data, they often struggle with complex interactions between features. They create axis-aligned splits that may not be the best way to model certain types of intricate relationships.\n",
    "\n",
    "* ***computationally intensive***: as tree depth increases, decision trees can become computationally expensive. Training and inference times may slow significantly, especially for very deep trees or large datasets, making them less efficient in some scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALGORITHM STEPS\n",
    "\n",
    "1. ***select the best feature to split***: the algorithm starts with the entire dataset and selects the best feature to split the data. It chooses the feature that provides the best separation based on impurity measures:\n",
    "    * *classification*: uses *gini index* or *entropy (information gain)*\n",
    "    * *regression*: uses *mean squared error (mse)* or *variance reduction*\n",
    "\n",
    "2. ***split the data***: the dataset is split into two *child nodes (binary split)*. Each split should reduce impurity, making the subgroups more homogeneous.\n",
    "\n",
    "3. ***recursively repeat***: the process continues recursively for each child node. This forms a tree-like structure until a stopping condition is met:\n",
    "    * maximum depth is reached.\n",
    "    * minimum number of samples per node.\n",
    "    * no further reduction in impurity.\n",
    "\n",
    "4. ***make predictions***: once the tree is built, it makes predictions by traversing from the root to a leaf node based on the feature values of the input. The class or value in the leaf node is the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION TREE "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

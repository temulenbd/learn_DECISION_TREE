{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Decision Tree is a supervised learning model used for both classification and regression tasks. It works by splitting the data into branches based on feature values, creating a tree-like structure where each internal node represents an outcome (class label or predictid value)\n",
    "\n",
    "In a Decision Tree algorithm, there is a tree like structure in which each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label. The paths from the root node to leaf node represent classification rules.\n",
    "\n",
    "* ***root node***: the starting point, containing all the data.\n",
    "* ***splitting***: the dataset is split based on the feature that provides the best separation (e.g., using *gini impurity* or *entropy* for classification).\n",
    "* ***decision nodes***: intermediate nodes that split the data further based on feature values.\n",
    "* ***leaf/terminal nodes***: the final nodes that provide the prediction (class label for classification or numerical value for regression).\n",
    "* ***pruning***: the removal of sub-nodes. It is the opposite process of splitting.\n",
    "* ***branch/sub-tree***: a sub-section of an entire tree is called a branch or sub-tree.\n",
    "* ***parent and child node***: a node, which is divided into sub-nodes is called the parent node of sub-nodes where sub-nodes are the children of a parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADVANTAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***easy to understand and interpret***: decision trees provide clear visualizations that make them highly interpretable. The structure of the tree allows for easy understanding of how decisions are made at each node, which can be particularly useful for non-experts.\n",
    "\n",
    "* ***non-linear relationships:***: capable of modeling non-linear relationships between features and the target variable, decision trees offer greater flexibility compared to linear models that assume a linear connection between inputs and outputs.\n",
    "\n",
    "* ***handling of numerical and categorical data***: both numerical and categorical data can be processed without the need for additional preprocessing or encoding, distinguishing decision trees from other machine learning models that require feature scaling or transformation.\n",
    "\n",
    "* ***robustness to outliers:***: the model is relatively insensitive to outliers. Since splits are made based on feature values that separate the majority of data, extreme values do not heavily influence the model‚Äôs decision-making process.\n",
    "\n",
    "* ***automatic feature selection***: decision trees inherently perform feature selection by choosing the most relevant features for creating splits in the data, which can lead to reduced dimensionality and enhanced model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISADVANTAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***prone to overfitting***: decision trees have a tendency to overfit, especially when the tree depth increases. A deep tree may capture noise and minor patterns, leading to poor generalization on new, unseen data.\n",
    "\n",
    "* ***instability***: decision trees can exhibit instability since slight changes in the data may result in a completely different structure. This is due to the greedy nature of the algorithm that selects the best split at each step, which can cause drastic changes in the final model.\n",
    "\n",
    "* ***bias towards dominant features***: the model can exhibit bias towards features with more categories or numerical splits. It may favor features that provide the most significant reduction in impurity, even if they are not the most informative in predicting the target variable.\n",
    "\n",
    "* ***difficulty modeling complex relationships***: while decision trees can handle non-linear data, they often struggle with complex interactions between features. They create axis-aligned splits that may not be the best way to model certain types of intricate relationships.\n",
    "\n",
    "* ***computationally intensive***: as tree depth increases, decision trees can become computationally expensive. Training and inference times may slow significantly, especially for very deep trees or large datasets, making them less efficient in some scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THEORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ENTROPHY & INFORMATION GAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`enthrophy`\n",
    "\n",
    "FORMULA:\n",
    "\n",
    "$$ ENTROPHY = H(S) = -\\sum_{i=1}^{n} p_i \\log_2 p_i $$\n",
    "‚Äã\n",
    "WHERE:\n",
    "\n",
    "- $ùëÜ$ = dataset  \n",
    "- $ùëù_ùëñ$ = proportion of class $ùëñ$\n",
    "\n",
    "*Entropy* measures the *impurity* in a dataset (in simple terms: *entropy* is a measure of uncertainty or disorder. Imagine you're in a room with a lot of different colored balls scattered randomly across the floor. If the balls are all the same color, there's no uncertainty ‚Äî you know exactly what color you'll pick. But if the balls are many different colors, there's more uncertainty because you can't predict the color you'll get.\n",
    "\n",
    "In data science, entropy is used to measure how *mixed* or *uncertain* a set of data is. The higher the entropy, the more unpredictable or disordered the data. If all data points are of one type (like all red balls), the entropy is low. But if the data points are evenly mixed (like red, blue, green, etc.), the entropy is high.\n",
    "\n",
    "In a decision tree or classification problem, entropy helps us decide which feature or attribute to use to split the data. We aim to split the data in a way that reduces entropy, making the data more organized and easier to classify.)\n",
    "\n",
    "* *low entropy*: a basket with only red apples (you know exactly what you will pick).\n",
    "* *high entropy*: a basket with red apples, green apples, and yellow apples mixed together (it's uncertain which color you will pick).\n",
    "\n",
    "EXAMPLE:\n",
    "\n",
    "*Given:* \n",
    "\n",
    "* 10 fruits in a basket: 6 apples (red), 4 oranges (orange).\n",
    "\n",
    "*Calculation*:\n",
    "* total number of fruits: \n",
    "    * $total = 10 (6 Apples + 4 Oranges)$\n",
    "\n",
    "* proportions of each class:  \n",
    "    * $p_{\\text{apple}} = 6 / 10 = 0.6$ - proportion of apples\n",
    "    * $p_{\\text{orange}} = 4 / 10 = 0.4$ - proportion of oranges\n",
    "* entrophy calculation:\n",
    "    * $H(S) = - \\left[ p_{\\text{Apple}} \\log_2 p_{\\text{Apple}} + p_{\\text{Orange}} \\log_2 p_{\\text{Orange}} \\right]$\n",
    "\n",
    "* substituting the values:\n",
    "    * $H(S) = - \\left[ 0.6 \\log_2 0.6 + 0.4 \\log_2 0.4 \\right]$\n",
    "    * We calculate each term:\n",
    "        * $0.6 \\log_2 0.6 \\approx 0.6 \\times -0.736 = -0.442$\n",
    "        * $0.4 \\log_2 0.4 \\approx 0.4 \\times -1.322 = -0.528$\n",
    "    * So, the entropy is:\n",
    "        * $H(S) = - \\left[ -0.442 + (-0.528) \\right]$\n",
    "        * $H(S) = 0.970 \\, \\text{bits}$\n",
    "\n",
    "*Result interpretation*:\n",
    "\n",
    "* The entropy of this dataset is *0.970 bits*, meaning there is some uncertainty (but not too high, because the distribution of Apples and Oranges is not perfectly even). If the dataset had only one type of fruit (all Apples or all Oranges), the entropy would have been *0*, indicating no uncertainty at all. The *higher the entropy*, the more mixed and uncertain the data is. The entropy of *0.970 bits* tells us there's moderate disorder (uncertainty) in the fruit basket, as we have two types, but not evenly distributed.\n",
    "\n",
    "`information gain`\n",
    "\n",
    "FORMULA:\n",
    "$$ IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v) $$\n",
    "\n",
    "WHERE:  \n",
    "- $IG(S, A)$ is the information gain from attribute $A$ for dataset $S$.  \n",
    "- $H(S)$ is the entropy of the original dataset $S$.  \n",
    "- $S_v$ is the subset of $S$ for which attribute $A$ has value $v$.  \n",
    "- $H(S_v)$ is the entropy of the subset $S_v$.  \n",
    "- $|S|$ is the total number of instances in the dataset $S$.  \n",
    "- $|S_v|$ is the number of instances in the subset $S_v$.  \n",
    "\n",
    "*Information Gain* is a measure of how much uncertainty is reduced when a dataset is split based on a particular attribute. In simple terms, *information gain* is the reduction in entropy (uncertainty) that results from splitting the dataset based on an attribute. It helps us choose the best attribute to split the data when building decision trees. (Imagine you're trying to organize a set of balls (like the earlier example) by color. Initially, the balls are scattered, and it‚Äôs hard to predict what color you'll pick ‚Äî the uncertainty (entropy) is high. Now, if you sort the balls into different piles based on their color, the uncertainty decreases. You now know which color you‚Äôll pick if you choose a specific pile. The *information gain* is how much the uncertainty (entropy) has been reduced by this sorting process.)\n",
    "\n",
    "In a decision tree, *information gain* helps us figure out which feature (or attribute) will best separate the data into different classes. A higher *information gain* means that splitting the dataset based on that feature will lead to more organized and predictable groups.\n",
    "\n",
    "* *low information gain*: if splitting the data by an attribute doesn‚Äôt reduce much uncertainty (for example, if the attribute doesn‚Äôt significantly separate the data), the *information gain* will be low. This means the feature is not useful for classification.\n",
    "* *high information gain*: if splitting the data by an attribute reduces a lot of uncertainty (for example, if the attribute leads to a very clear division of classes), the *information gain* will be high. This means the feature is very useful for classification.\n",
    "\n",
    "EXAMPLE:\n",
    "\n",
    "*Given:*\n",
    "\n",
    "* 10 fruits in a basket: 6 apples (red), 4 oranges (orange).\n",
    "    * Calculate the *information gain* of splitting the dataset by the feature *color* (red vs. orange)\n",
    "\n",
    "*Calculation*:\n",
    "* total number of fruits:  \n",
    "    * $ \\text{total} = 10 \\quad (6 \\text{ Apples} + 4 \\text{ Oranges}) $\n",
    "\n",
    "* entropy (before the split):\n",
    "    * $ H(S) = - \\left[ -0.442 + (-0.528) \\right] $  \n",
    "    * $ H(S) = 0.970 \\, \\text{bits} $\n",
    "\n",
    "* split the dataset by the feature *color* (there are two groups):\n",
    "    * *Group 1* Red: 6 apples\n",
    "    * *Group 2* Orange: 4 oranges\n",
    "\n",
    "* calculate entropy for each subset:\n",
    "    * *Group 1*: since all fruits in this group are apples, the entropy is 0 (no uncertainty)\n",
    "        * $ H(\\text{Red}) = 0 \\, \\text{bits} $\n",
    "    * *Group 2*: all fruits in this group are oranges, so the entropy is also 0:\n",
    "        * $ H(\\text{Orange}) = 0 \\, \\text{bits} $\n",
    "\n",
    "* calculate the weighted entropy after the split:\n",
    "    * $ H(S_{\\text{split}}) = \\frac{6}{10} \\cdot H(\\text{Red}) + \\frac{4}{10} \\cdot H(\\text{Orange}) $\n",
    "    \n",
    "    since both entropies are 0:\n",
    "    \n",
    "    * $ H(S_{\\text{split}}) = \\frac{6}{10} \\cdot 0 + \\frac{4}{10} \\cdot 0 = 0 \\, \\text{bits} $\n",
    "\n",
    "* calculate information gain:\n",
    "    * $ IG(S, \\text{color}) = H(S) - H(S_{\\text{split}}) $\n",
    "    \n",
    "    substitute the values:\n",
    "    \n",
    "    * $ IG(S, \\text{color}) = 0.970 - 0 = 0.970 \\, \\text{bits} $\n",
    "\n",
    "*Result Interpretation:*\n",
    "* The *information gain* from splitting the dataset by the color of the fruits is *0.970 bits*. This is the amount of uncertainty reduced by splitting the dataset based on the *color* attribute. Since the fruits are perfectly separated by color (all red fruits in one group and all orange fruits in another), the *information gain* is high, indicating that color is a very good feature for splitting the data. The uncertainty (entropy) in each group is 0, and the total uncertainty is completely eliminated by the split, which is why the *information gain* is equal to the original entropy of the dataset. In summary, *information gain* helps identify the best attribute to split the data, and a higher *information gain* indicates a more effective feature for making predictions. In this case, *color* is a very effective feature, as it leads to perfect separation of the apples and oranges.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GINI IMPURITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "FORMULA:\n",
    "\n",
    "$$ G(S) = 1 - \\sum_{i=1}^{n} p_i^2 $$\n",
    "\n",
    "WHERE:\n",
    "* $G(S)$ is the *gini impurity* of the dataset $S$\n",
    "* $p_i$ is the proportion of class $i$ in the dataset $S$\n",
    "* $n$ is the total number of distinct classes in the dataset $S$\n",
    "* $\\sum_{i=1}^{n} p_i^2$ is the sum of the squared proportions of each class in the dataset\n",
    "\n",
    "*Gini impurity* is a measure of how impure or mixed a dataset is, similar to entropy, but it focuses on the probability of a randomly selected element being misclassified. In simple terms, *gini impurity* is like looking into a box of colored balls. If all the balls are the same color, there‚Äôs no impurity ‚Äî you know exactly what color you'll pick. But if there are several different colors mixed together, it becomes more uncertain because you can't predict what color you'll choose.\n",
    "\n",
    "In a decision tree or classification problem, we use *gini impurity* to decide which feature or attribute to split the data on. The goal is to split the data in such a way that impurity is minimized, making the data easier to classify. By calculating *gini impurity* for different splits, we can choose the one that best organizes the data, leading to better classification accuracy.\n",
    "\n",
    "* *low gini impurity*: a basket with only red apples. You know exactly what you'll pick, so the *gini impurity* is 0.\n",
    "\n",
    "* *high gini impurity*: a basket with red apples, green apples, and yellow apples mixed together. It‚Äôs uncertain which color you‚Äôll pick, so the *gini impurity* is high.\n",
    "\n",
    "EXAMPLE:\n",
    "\n",
    "*Given:*\n",
    "\n",
    "- 10 fruits in a basket: 6 apples (red), 4 oranges (orange).\n",
    "- We want to calculate the **Gini Impurity** of the dataset before splitting it.\n",
    "\n",
    "*Calculation*:\n",
    "* total number of fruits:\n",
    "    * $ \\text{total} = 10 \\quad (6 \\text{ Apples} + 4 \\text{ Oranges}) $\n",
    "* proportions of each class:  \n",
    "    * $ p_{\\text{apple}} = \\frac{6}{10} = 0.6 \\quad \\text{(proportion of apples)} $\n",
    "    * $ p_{\\text{orange}} = \\frac{4}{10} = 0.4 \\quad \\text{(proportion of oranges)} $\n",
    "\n",
    "* gini impurity calculation (before the split):\n",
    "    * $ G(S) = 1 - \\left( (0.6)^2 + (0.4)^2 \\right) $\n",
    "\n",
    "    calculate each term:  \n",
    "    \n",
    "    * $ (0.6)^2 = 0.36 $ \n",
    "    * $ (0.4)^2 = 0.16 $\n",
    "\n",
    "    now, calculate the gini impurity:  \n",
    "    * $ G(S) = 1 - (0.36 + 0.16) = 1 - 0.52 = 0.48 $\n",
    "\n",
    "* split the dataset by the feature *color* (there are two groups):\n",
    "    * *Group 1* Red: 6 apples\n",
    "    * *Group 2* Orange: 4 oranges\n",
    "\n",
    "* calculate gini impurity for each subset:\n",
    "    * *Group 1*: since all fruits in this group are apples, the gini impurity is 0 (no uncertainty):\n",
    "        * $ G(\\text{Red}) = 1 - (1^2) = 0 $\n",
    "    * *Group 2*: all fruits in this group are oranges, so the gini impurity is also 0:\n",
    "        * $ G(\\text{Orange}) = 1 - (1^2) = 0 $\n",
    "\n",
    "* calculate the weighted gini impurity (after the split):\n",
    "    * $ G(S_{\\text{split}}) = \\frac{6}{10} \\cdot G(\\text{Red}) + \\frac{4}{10} \\cdot G(\\text{Orange}) $\n",
    "    \n",
    "    since both gini impurities are 0:\n",
    "    * $ G(S_{\\text{split}}) = \\frac{6}{10} \\cdot 0 + \\frac{4}{10} \\cdot 0 = 0 $\n",
    "\n",
    "* calculate *gini gain*:\n",
    "    * the ***Gini Gain*** is calculated by subtracting the weighted *gini impurity* after the split from the original *gini impurity*:\n",
    "        * $ GG(S, \\text{color}) = G(S) - G(S_{\\text{split}}) $\n",
    "        \n",
    "        substitute the values:\n",
    "        \n",
    "        * $ GG(S, \\text{color}) = 0.48 - 0 = 0.48 $\n",
    "\n",
    "*Result Interpretation:*\n",
    "\n",
    "The *gini gain* from splitting the dataset by the color of the fruits is *0.48*. This indicates the amount of impurity (uncertainty) reduced by splitting the dataset based on the *color* feature. Since the fruits are perfectly separated by color (all red fruits in one group and all orange fruits in another), the *gini gain* is high, indicating that *color* is a very good feature for splitting the data. The *gini impurity* in each group is 0, and the total impurity is completely eliminated by the split, which is why the *gini gain* is equal to the original *gini impurity* of the dataset. In summary, *gini impurity* is used to measure the *impurity* of the dataset before and after a split. A higher *gini gain* indicates a better split, as it shows a large reduction in impurity. In this case, *color* is a very effective feature for splitting the dataset because it leads to perfect separation of the apples and oranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ***select the best feature to split***: the algorithm starts with the entire dataset and selects the best feature to split the data. It chooses the feature that provides the best separation based on impurity measures:\n",
    "    * *classification*: uses *gini index* or *entropy (information gain)*\n",
    "    * *regression*: uses *mean squared error (mse)* or *variance reduction*\n",
    "\n",
    "2. ***split the data***: the dataset is split into two *child nodes (binary split)*. Each split should reduce impurity, making the subgroups more homogeneous.\n",
    "\n",
    "3. ***recursively repeat***: the process continues recursively for each child node. This forms a tree-like structure until a stopping condition is met:\n",
    "    * maximum depth is reached.\n",
    "    * minimum number of samples per node.\n",
    "    * no further reduction in impurity.\n",
    "\n",
    "4. ***make predictions***: once the tree is built, it makes predictions by traversing from the root to a leaf node based on the feature values of the input. The class or value in the leaf node is the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRARIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ***scikit-learn***: a widely used, general-purpose library for building decision trees. It is well-optimized, easy to use, and widely adopted in both industry and academia.\n",
    "    * `DecisionTreeClassifier` - for classification\n",
    "    * `DecisionTreeRegressor` - for regression\n",
    "    * `export_text` - for printing tree structure as text\n",
    "    * `export_graphviz` - for visualizing decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_text, export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ***xgboost***: an optimized gradient boosting library that builds decision trees in an efficient and scalable way. Popular for structured/tabular data.\n",
    "    * `XGBClassifier` - for classification\n",
    "    * `XGBRegressor` - for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install: pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ***TensorFlow Decision Forests***: library that provides state-of-the-art algorithms for training, serving, and interpreting decision forest models, including random forests and gradient boosted trees, within the TensorFlow ecosystem.\n",
    "    * `tfdf.keras.CartModel` - for simple decision trees\n",
    "    * `tfdf.keras.GradientBoostedTreesModel` - for boosted trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install: pip install tensorflow_decision_forests\n",
    "import tensorflow_decision_forests as tfdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ***criterion*** (Split Quality Measure): defines the function used to measure the quality of a split.Options:\n",
    "    * *gini* - uses gini impurity (default)\n",
    "    * *entropy* - uses entropy for information gain\n",
    "    * *log_loss* - suitable for classification with probability estimates\n",
    "\n",
    "    BEST PRACTICE: use cross-validation to test different criteria and select the one that improves accuracy.\n",
    "\n",
    "2. ***max_depth*** (Maximum Depth of the Tree): controls how deep the decision tree grows. Effect:\n",
    "    * *shallow tree* (small max_depth) prevents overfitting but may lead to underfitting.\n",
    "    * *deep tree* (large max_depth) can capture more patterns but may overfit.\n",
    "\n",
    "    BEST PRACTICE: tune using GridSearchCV for set None for full depth and use pruning techniques.\n",
    "\n",
    "3. ***min_samples_split*** (Minimum Samples Required to Split a Node): determines the minimum number of samples required to split an internal node. Effect:\n",
    "    * higher values reduce the tree's complexity and prevent overfitting.\n",
    "    * lower values allow deeper trees, increasing variance.\n",
    "\n",
    "    BEST PRACTICE: start with min_samples_split=2 and tune using cross-validation.\n",
    "\n",
    "4. ***min_samples_leaf*** (Minimum Samples per Leaf Note): specifies the minimum number of samples a leaf node must contain. Effect:\n",
    "    * higher values create smoother decision boundaries.\n",
    "    * lower values allow more splits, increasing overfitting risk.\n",
    "    \n",
    "    BEST PRACTICE: use values between 1-5% of the total dataset size to balance bias and variance.\n",
    "\n",
    "5. ***max_features*** (Number of Features Considering for Splitting): limits the number of features considered when looking for the best split. Options:\n",
    "    * *auto* or *sqrt* - uses sqrt(n_features), often best for classification\n",
    "    * *log2* - uses log2(n_features), offering more randomness\n",
    "    * *None* - uses all features (default)\n",
    "\n",
    "    BEST PRACTICE: use *sqrt* for classification and *log2* for high dimensional data.\n",
    "\n",
    "6. ***max_leaf_nodes*** (Maximum Number of Leaf Nodes): ristricts the number of leaf nodes in the tree. Effect:\n",
    "    * limits overfitting by reducing the tree size\n",
    "    * setting *None* allows unlimited leaves (default)\n",
    "\n",
    "    BEST PRACTICE: set a reasonable upper bound to prevent excessive branching\n",
    "\n",
    "7. ***min_impurity_decrease*** (Minimum Reduction in Impurity for a Split): a node will be split if the impurity decrease exceeds this threshold. Effect:\n",
    "    * prevents unnecessary splits, reducing overfitting.\n",
    "    * default is 0.0, meaning no restrictions.\n",
    "\n",
    "    BEST PRACTICE: use values like 0.001 to 0.01 to control complexity. \n",
    "\n",
    "8. ***splitter*** (Splitting Strategy): determines how nodes are split. Options:\n",
    "    * *best* - selects the best possible split (default)\n",
    "    * *random* - selects a random split for more randomness\n",
    "\n",
    "    BEST PRACTICE: use *best* for most cases and *random* when dealing with larg noisy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
